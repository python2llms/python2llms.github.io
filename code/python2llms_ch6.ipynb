{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c2c06b",
   "metadata": {},
   "source": [
    "© 2025 Yegor Tkachenko. \n",
    "\n",
    "Python, Deep Learning, and LLMs: A Crash Course for Complete Beginners.\n",
    "\n",
    "Selected code from *Chapter 6: Language models for text prediction*.\n",
    "\n",
    "Version: 2025-09-03. https://python2llms.org. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614991a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text string length: 146458\n",
      "Text:   Alice’s Adventures in Wonderland | Project Gutenberg\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***  Alice’s\n",
      "Adventures in Wonderland by Lewis Carroll THE MILLENNIUM\n",
      "FULCRUM EDITION 3.0  Contents   CHAPTER I.Down the Rabbit-\n",
      "Hole   CHAPTER II.The Pool of Tears   CHAPTER III.A\n",
      "Caucus-Race and a Long Tale   CHAPTER IV.The Rabbit Sends\n",
      "in a Little Bill   CHAPTER V.Advice from a Caterpillar\n",
      "CHAPTER VI.Pig and Pepper   CHAPTER VII.A Mad Tea-Party\n",
      "CHAPTER VIII.The Queen’s Croquet-Ground   CHAPTER IX.The\n",
      "Mock Turtle’s Story   CHAPTER X.The Lobster Quadrille\n",
      "CHAPTER XI.Who Stole the Tarts?\n"
     ]
    }
   ],
   "source": [
    "import requests # downloads\n",
    "from bs4 import BeautifulSoup # html parsing\n",
    "import re # regular expressions for text processing\n",
    "import textwrap # wrap print output\n",
    "\n",
    "# run the following code to download the text\n",
    "# or you can read in the provided version directly\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/11/11-h/11-h.htm'\n",
    "response = requests.get(url) # get web site's html\n",
    "response.encoding = 'utf-8' # correct encoding\n",
    "html_content = response.text # extract html content\n",
    "soup = BeautifulSoup(html_content,'html.parser') # parse html\n",
    "text = soup.get_text() # raw text from html as a string\n",
    "\n",
    "# remove repeated sequential spaces and newline characters\n",
    "text = re.sub(' +', ' ', text) \n",
    "text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "\n",
    "# save the file\n",
    "with open(\"alice.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(text)\n",
    "\n",
    "# read the saved file\n",
    "# with open(\"alice.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     text = file.read()\n",
    "\n",
    "print(\"Text string length:\", len(text))\n",
    "print(\"Text:\",textwrap.fill(text[:600], 58))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461c0d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " ['\\n', '\\r', ' ', '!', '(', ')', '*', ',', '-', '.', '0',\n",
      "'1', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G',\n",
      "'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S',\n",
      "'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c',\n",
      "'d', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o',\n",
      "'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|',\n",
      "'\\xa0', 'ù', '—', '‘', '’', '“', '”']\n",
      "Vocabulary size: 78\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "V = len(vocab) # vocabulary size\n",
    "print(\"Vocabulary:\\n\", textwrap.fill(str(vocab), 60))\n",
    "print(\"Vocabulary size:\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b007bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token - integer index mapping\n",
    "t2i = {tok:i for i,tok in enumerate(vocab)}\n",
    "i2t = {i:tok for i,tok in enumerate(vocab)}\n",
    "\n",
    "# functions to convert between integers and strings\n",
    "str2ind = lambda s: [t2i[t] for t in s] \n",
    "ind2str = lambda l: ''.join([i2t[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f17e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she ought to have wondered at this, but at the time it all\n",
      "seemed  quite natural); but when the Rabb\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(text[1500:1600], 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa9ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index encoding of a string:\n",
      " [62, 51, 48, 2, 58, 64, 50, 51, 63, 2, 63, 58, 2, 51, 44,\n",
      "65, 48, 2, 66, 58, 57, 47, 48, 61, 48, 47, 2, 44, 63, 2, 63,\n",
      "51, 52, 62, 7, 2, 45, 64, 63, 2, 44, 63, 2, 63, 51, 48, 2,\n",
      "63, 52, 56, 48, 2, 52, 63, 2, 44, 55, 55, 2, 62, 48, 48, 56,\n",
      "48, 47, 1, 0, 60, 64, 52, 63, 48, 2, 57, 44, 63, 64, 61, 44,\n",
      "55, 5, 14, 2, 45, 64, 63, 2, 66, 51, 48, 57, 2, 63, 51, 48,\n",
      "2, 33, 44, 45, 45]\n"
     ]
    }
   ],
   "source": [
    "print(\"Index encoding of a string:\\n\", \n",
    "    textwrap.fill(str( str2ind(text[1500:1600]) ), 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c76c7171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she ought to have wondered at this, but at the time it all\n",
      "seemed  quite natural); but when the Rabb\n"
     ]
    }
   ],
   "source": [
    "# integers back to string\n",
    "print( textwrap.fill(ind2str(str2ind(text[1500:1600])), 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c69af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: a b r a c a d a b r a _ r a\n",
      "Vocabulary: ['_', 'a', 'b', 'c', 'd', 'r']\n",
      "\n",
      "Step 2: a b ra c a d a b ra _ ra\n",
      "Vocabulary: ['_', 'a', 'b', 'c', 'd', 'r', 'ra']\n",
      "\n",
      "Step 3: ab ra c a d ab ra _ ra\n",
      "Vocabulary: ['_', 'a', 'b', 'c', 'd', 'r', 'ra', 'ab']\n",
      "\n",
      "Step 4: abra c a d abra _ ra\n",
      "Vocabulary: ['_', 'a', 'b', 'c', 'd', 'r', 'ra', 'ab',\n",
      "'abra']\n",
      "\n",
      "Step 5: abrac a d abra _ ra\n",
      "Vocabulary: ['_', 'a', 'b', 'c', 'd', 'r', 'ra', 'ab',\n",
      "'abra', 'abrac']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# frequency of adjacent token pairs\n",
    "def get_freq(sequence):\n",
    "    pairs = Counter()\n",
    "    for i in range(len(sequence) - 1):\n",
    "        pair = (sequence[i], sequence[i + 1])\n",
    "        pairs[pair] += 1\n",
    "    return pairs\n",
    "\n",
    "# merge the most frequent token pair\n",
    "def merge_tokens(sequence, pair_to_merge):\n",
    "    a, b = pair_to_merge\n",
    "    merged_token = a + b\n",
    "    i = 0\n",
    "    new_sequence = []\n",
    "    while i < len(sequence):\n",
    "        if i < len(sequence) - 1 and sequence[i] == a and sequence[i + 1] == b:\n",
    "            new_sequence.append(merged_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_sequence.append(sequence[i])\n",
    "            i += 1\n",
    "    return new_sequence\n",
    "\n",
    "# example\n",
    "sequence = \"a b r a c a d a b r a _ r a\".split()\n",
    "vocab_bpe = sorted(set(sequence))\n",
    "V_bpe = 10  # target vocabulary size\n",
    "merge_rules = [] # to store merge history\n",
    "\n",
    "print(f\"Step 1: {' '.join(sequence)}\")\n",
    "print(f\"Vocabulary: {sorted(vocab_bpe)}\\n\")\n",
    "\n",
    "step = 2\n",
    "while len(vocab_bpe) < V_bpe:\n",
    "    stats = get_freq(sequence)\n",
    "    if not stats:\n",
    "        break\n",
    "    most_common = stats.most_common(1)[0][0]\n",
    "    merge_rules.append(most_common)\n",
    "    sequence = merge_tokens(sequence, most_common)\n",
    "    vocab_bpe.append(most_common[0] + most_common[1])\n",
    "    print(f\"Step {step}: {' '.join(sequence)}\")\n",
    "    print(textwrap.fill(f\"Vocabulary: {vocab_bpe}\",60)+\"\\n\")\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72f0be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned merge rules: [('r', 'a'), ('a', 'b'), ('ab', 'ra'),\n",
      "('abra', 'c')]\n",
      "New text: a b b r a _ r a\n",
      "Tokenized: ['ab', 'b', 'ra', '_', 'ra']\n"
     ]
    }
   ],
   "source": [
    "# apply learned merge rules to a new sequence\n",
    "def apply_merges(sequence, merge_rules):\n",
    "    for a, b in merge_rules:\n",
    "        i = 0\n",
    "        new_sequence = []\n",
    "        while i < len(sequence):\n",
    "            if i < len(sequence) - 1 and sequence[i] == a and sequence[i + 1] == b:\n",
    "                new_sequence.append(a + b)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_sequence.append(sequence[i])\n",
    "                i += 1\n",
    "        sequence = new_sequence  # update after each rule\n",
    "    return sequence\n",
    "\n",
    "print(textwrap.fill(\"Learned merge rules: \"+ \n",
    "    str(merge_rules),60))\n",
    "\n",
    "# tokenize new sequence\n",
    "new_seq = \"a b b r a _ r a\".split()\n",
    "tokenized = apply_merges(new_seq, merge_rules)\n",
    "\n",
    "print(\"New text:\", ' '.join(new_seq))\n",
    "print(\"Tokenized:\", tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174eff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8 bytes: b'\\xe5\\x88\\xab\\xe6\\x85\\x8c'\n",
      "Byte-level tokens: [229, 136, 171, 230, 133, 140]\n",
      "Decoded string: 别慌\n"
     ]
    }
   ],
   "source": [
    "# example: \"Don't panic\" (biè huāng) in Chinese\n",
    "chi = \"别慌\"\n",
    "\n",
    "utf8_bytes = chi.encode('utf-8')\n",
    "print(\"UTF-8 bytes:\", utf8_bytes)\n",
    "\n",
    "# convert each byte into an integer token (0-255)\n",
    "seq = list(utf8_bytes)\n",
    "print(\"Byte-level tokens:\", seq)\n",
    "\n",
    "# decode back (for illustration)\n",
    "decoded = bytes(seq).decode('utf-8')\n",
    "print(\"Decoded string:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64bafc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['H' 'e' 'l' 'l' 'o' '.']\n",
      " ['i' 'd' ' ' 'i' 't' '\\n']\n",
      " ['!' ' ' 'W' 'h' 'y' '?']\n",
      " ['a' 'r' 'e' ' ' 'y' 'o']]\n",
      "(4, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_raw = np.array([\n",
    "[\"H\", \"e\", \"l\", \"l\", \"o\", \".\"],\n",
    "[\"i\", \"d\", \" \", \"i\", \"t\", \"\\n\"],\n",
    "[\"!\", \" \", \"W\", \"h\", \"y\", \"?\"],\n",
    "[\"a\", \"r\", \"e\", \" \", \"y\", \"o\"],\n",
    "])\n",
    "print(X_raw)\n",
    "print(X_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8fc4308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23 48 55 55 58  9]\n",
      " [52 47  2 52 63  0]\n",
      " [ 3  2 38 51 68 15]\n",
      " [44 61 48  2 68 58]]\n"
     ]
    }
   ],
   "source": [
    "B, S = X_raw.shape\n",
    "X_ind = np.zeros((B,S), dtype=int)\n",
    "for b in range(B):\n",
    "    for s in range(S):\n",
    "        X_ind[b,s] = t2i[X_raw[b,s]] # token to index\n",
    "\n",
    "print(X_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b4ee9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6, 2)\n",
      "[[[ 0.60487573  0.30054313]\n",
      "  [ 0.10892936 -0.03912716]\n",
      "  [ 0.43816732  0.18814676]\n",
      "  [ 0.43816732  0.18814676]\n",
      "  [ 0.48026069 -1.08525169]\n",
      "  [ 0.98434258 -0.83544737]]\n",
      "\n",
      " [[-0.92767682 -0.71360919]\n",
      "  [ 0.61275229  0.25506099]\n",
      "  [-0.26613444 -0.64890071]\n",
      "  [-0.92767682 -0.71360919]\n",
      "  [ 0.66565147  1.07865774]\n",
      "  [ 0.12715784  1.40189088]]\n",
      "\n",
      " [[ 1.56626757 -2.09137019]\n",
      "  [-0.26613444 -0.64890071]\n",
      "  [ 0.15188388  1.08328312]\n",
      "  [ 0.07300425  0.73492972]\n",
      "  [-0.30719913  0.52414483]\n",
      "  [ 0.55931017  0.4740131 ]]\n",
      "\n",
      " [[ 2.0691885  -0.30205227]\n",
      "  [ 0.04509789  0.8590863 ]\n",
      "  [ 0.10892936 -0.03912716]\n",
      "  [-0.26613444 -0.64890071]\n",
      "  [-0.30719913  0.52414483]\n",
      "  [ 0.48026069 -1.08525169]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(999)\n",
    "\n",
    "E = 2 # embedding size\n",
    "\n",
    "# array of all embeddings shaped as\n",
    "# Alice in Wonderland vocab size V x embedding size E\n",
    "token_embeddings = np.random.normal(size=(V, E)) \n",
    "\n",
    "# array of token indices to array of embeddings\n",
    "X_emb = token_embeddings[X_ind]\n",
    "print(X_emb.shape)\n",
    "print(X_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0771647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12715784, 1.40189088])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3beffad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "[[ 0.50912383 -0.2138316 ]\n",
      " [-0.11932108  0.10991509]\n",
      " [ 0.29618872  0.01268331]\n",
      " [ 0.35502381 -0.11535012]]\n"
     ]
    }
   ],
   "source": [
    "# average embedding within each sequence\n",
    "X_avg_emb = np.mean(X_emb, axis=1)\n",
    "\n",
    "print(X_avg_emb.shape)\n",
    "print(X_avg_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acc40b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 78)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# layer params\n",
    "W = np.random.normal(size=(E, V)) \n",
    "b = np.random.normal(size=(1, V)) \n",
    "# note that bias is commonly initialized to zero\n",
    "\n",
    "# BxV array of logits / utilities\n",
    "X_logits = X_avg_emb.dot(W) + b\n",
    "\n",
    "print(X_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdb12789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 78)\n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def row_wise_softmax(x):\n",
    "    # subtract the maximum value for numerical stability\n",
    "    # this operation does not affect the resulting probability value \n",
    "    # because it is equivalent to dividing numerator and denominator\n",
    "    # by the same exp(max) value\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    # divide by the sum of exponentials for normalization\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "X_prob = row_wise_softmax(X_logits)\n",
    "\n",
    "print(X_prob.shape)\n",
    "\n",
    "# to verify probs. sum to 1 across full vocab.\n",
    "print(X_prob.sum(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3616b3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61  0 10 55]\n"
     ]
    }
   ],
   "source": [
    "samples = np.array([\n",
    "    np.random.choice(X_prob.shape[1], p=row) for row in X_prob\n",
    "])\n",
    "\n",
    "print(samples)  # e.g., array([1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0bad0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 50 75 61]\n"
     ]
    }
   ],
   "source": [
    "# row-wise cumulative probabilities\n",
    "# shape: (batch size B, vocab size V)\n",
    "cumulative = np.cumsum(X_prob, axis=1)  \n",
    "\n",
    "# one uniform random number per row\n",
    "u = np.random.rand(B)\n",
    "\n",
    "# compare uniform draw with cumulative distribution to sample\n",
    "samples = (cumulative < u[:, None]).sum(axis=1)\n",
    "\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5ea5ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.61284783 5.0697951  4.55075799 5.66530036]\n",
      "4.724675319705576\n"
     ]
    }
   ],
   "source": [
    "# continuation of sequences in X_raw\n",
    "y_raw = np.array([\n",
    "[\" \"],\n",
    "[\"f\"],\n",
    "[\"\\n\"],\n",
    "[\"u\"]\n",
    "])\n",
    "\n",
    "# indices\n",
    "y_ind = np.zeros((B,1), dtype=int)\n",
    "for b in range(B):\n",
    "    y_ind[b,0] = t2i[y_raw[b,0]]\n",
    "\n",
    "# cross entropy for each batch sequence\n",
    "# i.e., negative log probability assigned by the model \n",
    "# to the actual realized next character\n",
    "CE_s = -np.log(X_prob[np.arange(B), y_ind.flatten()])\n",
    "print(CE_s)\n",
    "\n",
    "# average cross entropy\n",
    "CE = np.mean(CE_s)\n",
    "print(CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5edbce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fastest available hardware for deep learning: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' # gpu\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(\"Fastest available hardware for deep learning:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21fee50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 character indices: tensor([ 0,  0, 16, 55, 52])\n"
     ]
    }
   ],
   "source": [
    "# integer value pytorch tensor \n",
    "# containing integer encoding of the full \n",
    "# Alice in Wonderland text\n",
    "data = torch.tensor(str2ind(text), dtype=torch.long)  \n",
    "n = int(0.8*data.shape[0])\n",
    "data_train = data[:n]\n",
    "data_test = data[n:]\n",
    "\n",
    "print(\"First 5 character indices:\", data_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adad5457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[62, 64, 61, 59, 61, 52],\n",
       "         [ 2, 76, 27, 48, 63,  2],\n",
       "         [48,  2, 53, 64, 56, 59],\n",
       "         [58, 61,  2, 63, 51, 48]], device='mps:0'),\n",
       " tensor([62, 64, 52,  1], device='mps:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 4 # batch size - number of token sequences\n",
    "S = 6 # input sequence length\n",
    "\n",
    "def get_batch(split):\n",
    "    # get text sequences from random starting points\n",
    "    if split == 'train':\n",
    "        d = data_train\n",
    "    if split == \"test\":\n",
    "        d = data_test\n",
    "\n",
    "    # sampling starting points for sequences \n",
    "    start_ind = torch.randint(d.shape[0] - S, (B,)) \n",
    "    # (d.shape[0] - S) is 1 above the largest integer to be drawn\n",
    "\n",
    "    X_ind = torch.stack([d[i:i+S] for i in start_ind])\n",
    "    y_ind = torch.stack([d[i+S] for i in start_ind])\n",
    "    X_ind, y_ind = X_ind.to(device), y_ind.to(device)\n",
    "    return X_ind, y_ind\n",
    "\n",
    "get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a0f93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "torch.manual_seed(999)\n",
    "\n",
    "# V is Alice in Wonderland vocabulary size\n",
    "B = 4  # batch size - number of token sequences\n",
    "S = 8  # input sequence length\n",
    "E = 16 # embedding size\n",
    "\n",
    "# model definition\n",
    "class AvgEmbeddingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, V, E):\n",
    "        super().__init__() \n",
    "        # this calls the __init__ of the parent class nn.Module\n",
    "\n",
    "        # V x E array of all embedding vectors\n",
    "        self.token_embeddings = nn.Embedding(V, E)\n",
    "\n",
    "        # a fully-connected layer of V neurons\n",
    "        # each neuron takes E input values\n",
    "        self.output_layer = nn.Linear(E, V)\n",
    "\n",
    "    def forward(self, X_ind):\n",
    "        # X_ind is B x S array \n",
    "        # sequences of integers\n",
    "      \n",
    "        # X_emb is B x S x E array\n",
    "        # sequences of embeddings\n",
    "        X_emb = self.token_embeddings(X_ind)  \n",
    "\n",
    "        # X_emb_avg is B x E array\n",
    "        # average within-sequence embeddings\n",
    "        X_emb_avg = torch.mean(X_emb, 1)\n",
    "\n",
    "        # X_logits is B x V array\n",
    "        # utilities / logits for possible next characters\n",
    "        # row-wise softmax would generate probabilities\n",
    "        X_logits = self.output_layer(X_emb_avg)\n",
    "        return X_logits\n",
    "\n",
    "    def next_token_prob(self, X_logits):\n",
    "        # X_prob is B x V array of probabilities\n",
    "        # for possible next characters\n",
    "        X_prob = F.softmax(X_logits, dim=-1) # B x V\n",
    "        return X_prob\n",
    "\n",
    "    def loss(self, X_logits, y_ind):\n",
    "        # cross-entropy loss\n",
    "        return F.cross_entropy(X_logits, y_ind)\n",
    "\n",
    "net = AvgEmbeddingModel(V, E)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d807b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters (embeddings + output layer)\n",
      " in AvgEmbeddingModel: 2574\n"
     ]
    }
   ],
   "source": [
    "print('Number of trainable parameters (embeddings + output layer)\\n in AvgEmbeddingModel:', \n",
    "  sum(p.numel() for p in net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2c10e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Train loss = 4.4215, Test loss = 4.3955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300: Train loss = 3.0962, Test loss = 3.1353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600: Train loss = 3.1422, Test loss = 3.1990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 900: Train loss = 3.0638, Test loss = 3.0082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1200: Train loss = 2.9821, Test loss = 3.0287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500: Train loss = 2.8936, Test loss = 3.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1800: Train loss = 3.0589, Test loss = 3.0315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2100: Train loss = 2.9676, Test loss = 2.9623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2400: Train loss = 2.9257, Test loss = 3.0203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2700: Train loss = 2.8846, Test loss = 2.9998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2999: Train loss = 2.9716, Test loss = 2.9562\n",
      "\n",
      "\t Elapsed time = 0.32 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# training settings\n",
    "learning_rate = 1e-2 # 0.01\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "eval_iters = 200\n",
    "\n",
    "# AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# train and test set evaluation\n",
    "@torch.no_grad() # this decorator disables gradient calculation within the function\n",
    "def evaluate(net):\n",
    "    record = {}\n",
    "    net.eval() # evaluation mode (e.g., turns off dropout when present)\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X_ind, Y_ind = get_batch(split)\n",
    "            X_logits = net(X_ind)\n",
    "            loss = net.loss(X_logits, Y_ind)\n",
    "            losses[k] = loss.item()\n",
    "        record[split] = losses.mean()\n",
    "    net.train() # train mode\n",
    "    return record\n",
    "\n",
    "# training loop\n",
    "start_time = time.time()\n",
    "for i in range(max_iters):\n",
    "    \n",
    "    # regularly evaluate the loss on train and test sets\n",
    "    if i % eval_interval == 0 or i == max_iters - 1:\n",
    "        eval_ = evaluate(net)\n",
    "        print(f\"Step {i}: \"\n",
    "              f\"Train loss = {eval_['train']:.4f}, \"\n",
    "              f\"Test loss = {eval_['test']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    X_batch, y_batch = get_batch('train')\n",
    "\n",
    "    # evaluate the loss and update the parameters\n",
    "    X_logits = net(X_batch)\n",
    "    loss = net.loss(X_logits, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n\\t Elapsed time = {elapsed/60:.2f} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caed1c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test set cross-entropy (CE): 2.9562\n",
      "Probability of picking the correct next token: \n",
      "> Neural net (exp(-CE)): 0.052\n",
      "> Uniform random guess (1/V): 0.0128\n"
     ]
    }
   ],
   "source": [
    "print(\"Final test set cross-entropy (CE):\", \n",
    "    float(f\"{eval_['test']:.4f}\"))\n",
    "print(\"Probability of picking the correct next token: \")\n",
    "print(\"> Neural net (exp(-CE)):\", \n",
    "    float(f\"{torch.exp(-eval_['test']):.4f}\"))\n",
    "print(\"> Uniform random guess (1/V):\", round(1/V,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64865cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Areie se npryataw  nidtha ,sgeetil We hens mof uoot aotn—ug\n",
      "Wlhieatigs etb hhro seo natpd ?h tet olnaadtak nnatlheat”! ’\n",
      "tohdahey rhws  ehWeh pesi  oano tu e mhhMsoeok sjn—dhee nbas\n",
      "Ao e  sofhsol r entihoH use,laan  niatgmhlel r ouosa\n",
      "irupnstwe ehrerud lo, hb etrrobe“f  hror mefelann role\n",
      "oonttohi,: a wts tnae ihttdr Kli iivttns si aaatnins yohe\n",
      "aAt tseireon w siomei nt eshd“a xertpab esitrfs eha out‘vnss\n",
      "a g  i“ygo ‘e’’ oroattae peyntt oehse inthtoeng zin\n",
      "TEeithir  nta  edohont ehpa rohtsi\n"
     ]
    }
   ],
   "source": [
    "def generate_text(net, n_new_tokens):\n",
    "    X_ind = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    for i in range(n_new_tokens):\n",
    "        # crop to sequence size\n",
    "        X_ind_crop = X_ind[:, -S:] # 1 x S\n",
    "        X_logits = net(X_ind_crop) # 1 x V\n",
    "        X_prob = net.next_token_prob(X_logits) # 1 x V\n",
    "        X_ind_next = torch.multinomial(X_prob, num_samples=1) # 1 x 1\n",
    "        X_ind = torch.cat((X_ind, X_ind_next), dim=1) # 1 x S+1\n",
    "    return ind2str(X_ind[0].tolist())\n",
    "\n",
    "net.eval()\n",
    "s = generate_text(net, n_new_tokens=500)\n",
    "print(textwrap.fill(s, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c424ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something happens before func runs\n",
      "Hello!\n",
      "Something happens after func runs\n"
     ]
    }
   ],
   "source": [
    "def my_decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Something happens before func runs\")\n",
    "        func()\n",
    "        print(\"Something happens after func runs\")\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def say_hello():\n",
    "    print(\"Hello!\")\n",
    "\n",
    "say_hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d8dc8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random batch: tensor([[[-0.2528,  1.4072],\n",
      "         [ 0.2910,  1.0365],\n",
      "         [-0.9816, -3.4219],\n",
      "         [ 1.4910,  0.2422],\n",
      "         [ 1.4832, -0.3704],\n",
      "         [ 0.0941,  2.1528]],\n",
      "\n",
      "        [[ 0.6271, -1.1666],\n",
      "         [-0.7862,  0.0759],\n",
      "         [-0.0086, -0.6568],\n",
      "         [-1.0011,  0.2992],\n",
      "         [ 0.6396, -1.0857],\n",
      "         [-1.6153,  1.5635]],\n",
      "\n",
      "        [[-1.7952,  0.6095],\n",
      "         [-0.7203,  0.6119],\n",
      "         [ 0.3259, -1.6059],\n",
      "         [-0.5272,  0.3401],\n",
      "         [-1.3832,  1.1149],\n",
      "         [-0.7776,  0.2738]],\n",
      "\n",
      "        [[ 0.9147, -1.1896],\n",
      "         [-0.7501, -1.5465],\n",
      "         [ 1.0044, -0.0986],\n",
      "         [ 1.3962, -0.9138],\n",
      "         [-1.1788, -0.6681],\n",
      "         [-0.3168,  0.9893]]])\n",
      "Batch shape: torch.Size([4, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# toy example\n",
    "torch.manual_seed(999)\n",
    "\n",
    "B = 4 # batch size (number of sequences)\n",
    "S = 6 # sequence length\n",
    "E = 2 # embedding vector length\n",
    "\n",
    "# random batch - sequences of embeddings\n",
    "X_emb = torch.randn(B, S, E) \n",
    "\n",
    "print(\"Random batch:\", X_emb)\n",
    "print(\"Batch shape:\", X_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd0c0383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3542,  0.1744],\n",
       "        [-0.3575, -0.1618],\n",
       "        [-0.8129,  0.2240],\n",
       "        [ 0.1783, -0.5712]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_emb_avg = torch.mean(X_emb, dim=1) # B x E\n",
    "X_emb_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d6f7b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3542,  0.1744],\n",
       "        [-0.3575, -0.1618],\n",
       "        [-0.8129,  0.2240],\n",
       "        [ 0.1783, -0.5712]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights - vector of 6 scalars of 1/6\n",
    "w = torch.ones(S) / S\n",
    "\n",
    "# dot product\n",
    "X_emb_avg = w @ X_emb \n",
    "X_emb_avg\n",
    "\n",
    "# due to PyTorch broadcasting along batch dimension:\n",
    "# S  @  B x S x E  >>>  B x S  @  B x S x E  >>>  B x E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0b69373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3542,  0.1744],\n",
       "        [-0.3575, -0.1618],\n",
       "        [-0.8129,  0.2240],\n",
       "        [ 0.1783, -0.5712]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(S)\n",
    "\n",
    "# softmax - gives equal probabilities\n",
    "w = torch.exp(w)\n",
    "w = w / w.sum()\n",
    "\n",
    "# broadcasting magic again\n",
    "# S  @  B x S x E  >>>  B x E\n",
    "X_emb_avg = w @ X_emb \n",
    "X_emb_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b83b67c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(999)\n",
    "\n",
    "B, S, E = 4, 6, 2\n",
    "\n",
    "# input data\n",
    "X_emb = torch.randn(B, S, E)\n",
    "\n",
    "# attention parameters\n",
    "m_1 = m_2 = E # setting key/query and value matrix sizes\n",
    "W_q = torch.randn(E, m_1)\n",
    "W_k = torch.randn(E, m_1)\n",
    "W_v = torch.randn(E, m_2)\n",
    "\n",
    "# attention forward computation\n",
    "X_q = X_emb @ W_q  # B x S x m_1\n",
    "X_k = X_emb @ W_k  # B x S x m_1\n",
    "X_v = X_emb @ W_v  # B x S x m_2\n",
    "\n",
    "W = X_q @ X_k.transpose(-2, -1) # B x S x S\n",
    "W = W / (m_1**0.5)\n",
    "W = torch.softmax(W, dim=-1)\n",
    "\n",
    "X_out = W @ X_v # B x S x m_2  \n",
    "print(X_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff4cb6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[46, 44, 57, 75, 63,  2],\n",
       "         [61, 62, 48, 55, 49,  1],\n",
       "         [62,  2, 44, 62,  2, 62],\n",
       "         [77,  2, 16, 55, 52, 46]], device='mps:0'),\n",
       " tensor([63,  0, 63, 48], device='mps:0'))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(999)\n",
    "get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d812bacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_ind: tensor([[46, 44, 57, 75, 63,  2],\n",
      "        [61, 62, 48, 55, 49,  1],\n",
      "        [62,  2, 44, 62,  2, 62],\n",
      "        [77,  2, 16, 55, 52, 46]], device='mps:0')\n",
      "Y_ind: tensor([[44, 57, 75, 63,  2, 63],\n",
      "        [62, 48, 55, 49,  1,  0],\n",
      "        [ 2, 44, 62,  2, 62, 63],\n",
      "        [ 2, 16, 55, 52, 46, 48]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(999)\n",
    "\n",
    "B = 4    # number of token sequences\n",
    "S = 6    # input sequence length\n",
    "\n",
    "def get_batch(split):\n",
    "    # get text sequences from random starting points\n",
    "    if split == 'train':\n",
    "        d = data_train\n",
    "    if split == 'test':\n",
    "        d = data_test\n",
    "\n",
    "    start_ind = torch.randint(d.shape[0] - S, (B,)) \n",
    "    X_ind = torch.stack([d[i:i+S] for i in start_ind])\n",
    "    \n",
    "    # NOTE THE CHANGE:\n",
    "    Y_ind = torch.stack([d[i+1:i+S+1] for i in start_ind]) \n",
    "\n",
    "    X_ind, Y_ind = X_ind.to(device), Y_ind.to(device)\n",
    "    return X_ind, Y_ind\n",
    "\n",
    "X_ind, Y_ind = get_batch('train')\n",
    "\n",
    "print(\"X_ind:\",X_ind)\n",
    "print(\"Y_ind:\",Y_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3aedb3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(63, device='mps:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ind[0,:3] # first 3 elements in the first sampled sequence\n",
    "Y_ind[0,3]  # 4th element in that sequence, a valid prediction target for the first sequence\n",
    "X_ind[0,4]  # this is the same as Y_ind[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a704418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(999)\n",
    "B, S, E = 4, 6, 2  # batch size, sequence length, embd size\n",
    "X_emb = torch.randn(B, S, E)\n",
    "X_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a26f1c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2528,  1.4072],\n",
       "        [ 0.2910,  1.0365],\n",
       "        [-0.9816, -3.4219],\n",
       "        [ 1.4910,  0.2422],\n",
       "        [ 1.4832, -0.3704],\n",
       "        [ 0.0941,  2.1528]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_emb[0] # first sequence of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7c9f29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2528,  1.4072])\n",
      "tensor([0.0191, 1.2218])\n",
      "tensor([-0.3144, -0.3261])\n",
      "tensor([ 0.1369, -0.1840])\n",
      "tensor([ 0.4062, -0.2213])\n",
      "tensor([0.3542, 0.1744])\n"
     ]
    }
   ],
   "source": [
    "for i in range(S): \n",
    "    print(torch.mean(X_emb[0][:i+1,:],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b58bd951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the lower triangular matrix\n",
    "tril = torch.tril( torch.ones(S, S) )\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "403b55c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize its rows to sum to 1\n",
    "W = tril / tril.sum(1, keepdim=True)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "693251e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2528,  1.4072],\n",
       "        [ 0.0191,  1.2218],\n",
       "        [-0.3144, -0.3261],\n",
       "        [ 0.1369, -0.1840],\n",
       "        [ 0.4062, -0.2213],\n",
       "        [ 0.3542,  0.1744]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product to get the averages\n",
    "W @ X_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bc8ea96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., -inf],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.ones(S, S) # arbitrary weight matrix\n",
    "mask = torch.tril(torch.ones(S, S))\n",
    "W = W.masked_fill(mask == 0, float('-inf')) \n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "752c5cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.softmax(W, dim=-1)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d22406b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2528,  1.4072],\n",
       "        [ 0.0191,  1.2218],\n",
       "        [-0.3144, -0.3261],\n",
       "        [ 0.1369, -0.1840],\n",
       "        [ 0.4062, -0.2213],\n",
       "        [ 0.3542,  0.1744]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W @ X_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5793fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(999)\n",
    "\n",
    "B, S, E = 4, 6, 2\n",
    "\n",
    "# input data\n",
    "X_emb = torch.randn(B, S, E)\n",
    "\n",
    "# attention parameters\n",
    "m_1 = m_2 = E # setting key/query and value matrix sizes\n",
    "W_q = torch.randn(E, m_1)\n",
    "W_k = torch.randn(E, m_1)\n",
    "W_v = torch.randn(E, m_2)\n",
    "\n",
    "# attention forward computation\n",
    "X_q = X_emb @ W_q  # B x S x m_1\n",
    "X_k = X_emb @ W_k  # B x S x m_1\n",
    "X_v = X_emb @ W_v  # B x S x m_2\n",
    "\n",
    "W = X_q @ X_k.transpose(-2, -1)  # B x S x S\n",
    "W = W / (m_1**0.5)\n",
    "\n",
    "# causal filter\n",
    "\n",
    "# (S, S) lower triangular mask\n",
    "mask = torch.tril(torch.ones(S, S))  \n",
    "W = W.masked_fill(mask == 0, float('-inf'))\n",
    "W = torch.softmax(W, dim=-1)\n",
    "\n",
    "# the mask needs to be broadcasted to (B, S, S) if necessary\n",
    "# where mask is 0, set to negative infty >>> 0 after exp() \n",
    "\n",
    "X_out = W @ X_v  # B x S x m_2  \n",
    "print(X_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be706661",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 64 # batch size -- number of sequences\n",
    "S = 256 # sequence length, tokens per sequence\n",
    "E = 64 # embedding size per token\n",
    "\n",
    "n_heads = 4 # number of parallel attention heads\n",
    "n_layers = 4 # number of transformer blocks\n",
    "dropout = 0.2 # dropout probability (regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a4ff64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    # causal self-attention module\n",
    "\n",
    "    def __init__(self, m):\n",
    "        super().__init__()\n",
    "        self.m = m # m_1 = m_2\n",
    "\n",
    "        self.key = nn.Linear(E, m, bias=False) # W_k\n",
    "        self.query = nn.Linear(E, m, bias=False) # W_q\n",
    "        self.value = nn.Linear(E, m, bias=False) # W_v\n",
    "        \n",
    "        # create a non-trainable lower triangular matrix\n",
    "        self.register_buffer('tril', \n",
    "            torch.tril(torch.ones(S, S)))\n",
    "        self.dropout = nn.Dropout(dropout) # regularization\n",
    "\n",
    "    def forward(self, X_emb):\n",
    "        B, S, m = X_emb.shape \n",
    "\n",
    "        X_k = self.key(X_emb)   # B x S x m\n",
    "        X_q = self.query(X_emb) # B x S x m\n",
    "        X_v = self.value(X_emb) # B x S x m\n",
    "\n",
    "        # B x S x m  @  B x m x S  >>>  B x S x S\n",
    "        W = X_q @ X_k.transpose(-2,-1) / self.m**0.5\n",
    "        \n",
    "        W = W.masked_fill(self.tril[:S, :S] == 0, \n",
    "                float('-inf'))   # B x S x S\n",
    "        W = F.softmax(W, dim=-1) # B x S x S\n",
    "        W = self.dropout(W)\n",
    "        \n",
    "        # B x S x S  @  B x S x m  >>>  B x S x m\n",
    "        X_out = W @ X_v\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f598c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # group of attention heads running in parallel\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(head_size) for i in range(n_heads)\n",
    "        ])\n",
    "        self.lin = nn.Linear(E, E)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X_emb):\n",
    "        X_out = torch.cat([\n",
    "            h(X_emb) for h in self.heads\n",
    "        ], dim=-1) # B x S x E\n",
    "        X_out = self.dropout(self.lin(X_out))\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ab8d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    # neural net of two fully connected layers \n",
    "\n",
    "    def __init__(self, E):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(E, 4*E),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*E, E),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, X_emb):\n",
    "        return self.layers(X_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc3826be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    # layer normalization\n",
    "\n",
    "    def __init__(self, D, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(D))\n",
    "        self.bias = nn.Parameter(torch.zeros(D))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * x_norm + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b8e8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    # key transformer's building block \n",
    "\n",
    "    def __init__(self, E, n_heads):\n",
    "        super().__init__()\n",
    "        # E and n_heads should be selected \n",
    "        # so that E / n_heads has 0 remainder\n",
    "        head_size = E // n_heads\n",
    "        self.mha = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(E)\n",
    "        self.lnorm1 = LayerNorm(E)\n",
    "        self.lnorm2 = LayerNorm(E)\n",
    "\n",
    "    def forward(self, X_emb):\n",
    "        # skip connections via addition of input to output\n",
    "        X_emb = X_emb + self.mha(self.lnorm1(X_emb)) \n",
    "        X_emb = X_emb + self.ffwd(self.lnorm2(X_emb))\n",
    "        return X_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a08296bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # a basic transformer architecture LLM\n",
    "\n",
    "    def __init__(self, V, E):\n",
    "        super().__init__()\n",
    "        \n",
    "        # an embedding for each token\n",
    "        self.token_embeddings = nn.Embedding(V, E)\n",
    "\n",
    "        # and an embedding for each sequence position\n",
    "        self.position_embeddings = nn.Embedding(S, E)\n",
    "\n",
    "        # sequence of repeated transformer blocks\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(E, \n",
    "            n_heads=n_heads) for i in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # output layer\n",
    "        self.output_lnorm = LayerNorm(E) \n",
    "        self.output_layer = nn.Linear(E, V) \n",
    "\n",
    "    def forward(self, X_ind):\n",
    "        B, S = X_ind.shape\n",
    "        tok_emb = self.token_embeddings(X_ind) # B x S x E\n",
    "        pos_emb = self.position_embeddings(\n",
    "            torch.arange(S, device=device)) # S x E\n",
    "        X_emb = tok_emb + pos_emb # B x S x E\n",
    "        X_emb = self.blocks(X_emb) # B x S x E\n",
    "        X_emb = self.output_lnorm(X_emb) # B x S x E\n",
    "        X_logits = self.output_layer(X_emb) # B x S x V\n",
    "        return X_logits\n",
    "\n",
    "    def loss(self, X_logits, Y_ind):\n",
    "        # X_logits is B x S x V; Y_ind is B x S\n",
    "        return F.cross_entropy(\n",
    "                    X_logits.view(-1, X_logits.shape[-1]), \n",
    "                    Y_ind.view(-1))\n",
    "\n",
    "    def next_token_prob(self, X_logits):\n",
    "        # X_logits is B x S x V\n",
    "        X_logits = X_logits[:, -1, :] # B x V \n",
    "        X_prob = F.softmax(X_logits, dim=-1) # B x V\n",
    "        return X_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e4622f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, LayerNorm):\n",
    "        nn.init.ones_(module.scale)\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ddf04522",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(999)\n",
    "net = Transformer(V, E)\n",
    "net.apply(init_weights)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e10f94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.225742 million parameters\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in net.parameters())/1e6, 'million parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d034f700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Train loss = 4.3803, Test loss = 4.3808, \n",
      "\t Elapsed time = 0.00 mins, 0 % complete\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000: Train loss = 1.6440, Test loss = 1.7307, \n",
      "\t Elapsed time = 5.19 mins, 10 % complete\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000: Train loss = 1.2971, Test loss = 1.5063, \n",
      "\t Elapsed time = 9.55 mins, 20 % complete\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9000: Train loss = 1.1784, Test loss = 1.4534, \n",
      "\t Elapsed time = 13.89 mins, 30 % complete\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12000: Train loss = 1.1093, Test loss = 1.4398, \n",
      "\t Elapsed time = 18.21 mins, 40 % complete\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15000: Train loss = 1.0628, Test loss = 1.4304, \n",
      "\t Elapsed time = 22.49 mins, 50 % complete\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18000: Train loss = 1.0273, Test loss = 1.4260, \n",
      "\t Elapsed time = 26.84 mins, 60 % complete\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21000: Train loss = 1.0010, Test loss = 1.4223, \n",
      "\t Elapsed time = 31.12 mins, 70 % complete\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24000: Train loss = 0.9772, Test loss = 1.4253, \n",
      "\t Elapsed time = 35.43 mins, 80 % complete\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27000: Train loss = 0.9587, Test loss = 1.4209, \n",
      "\t Elapsed time = 39.76 mins, 90 % complete\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 29999: Train loss = 0.9412, Test loss = 1.4215, \n",
      "\t Elapsed time = 44.12 mins, 100 % complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_iters = 30000\n",
    "eval_interval = 3000\n",
    "learning_rate = 1e-4 # 0.0001\n",
    "\n",
    "optimizer = torch.optim.AdamW(net.parameters(), \n",
    "    lr=learning_rate, weight_decay=0.1)\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(max_iters):\n",
    "    \n",
    "    # evaluation\n",
    "    if i % eval_interval == 0 or i == max_iters - 1:\n",
    "        # elapsed time in seconds\n",
    "        elapsed = time.time() - start_time  \n",
    "        # reusing the eval function from earlier\n",
    "        eval_ = evaluate(net) \n",
    "        print(f\"Step {i}: \"\n",
    "            f\"Train loss = {eval_['train']:.4f}, \"\n",
    "            f\"Test loss = {eval_['test']:.4f}, \" \n",
    "            f\"\\n\\t Elapsed time = {elapsed/60:.2f} mins, \"\n",
    "            f\"{100*(i+1)/max_iters:.0f} % complete\\n\")\n",
    "\n",
    "    # training\n",
    "    X_batch, Y_batch = get_batch('train')\n",
    "    X_logits = net(X_batch)\n",
    "    loss = net.loss(X_logits, Y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11237fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test set cross-entropy (CE): 1.4215\n",
      "Probability of picking the correct next token: \n",
      "> Neural net (exp(-CE)): 0.2413\n",
      "> Uniform random guess (1/V): 0.0128\n"
     ]
    }
   ],
   "source": [
    "print(\"Final test set cross-entropy (CE):\", \n",
    "    float(f\"{eval_['test']:.4f}\"))\n",
    "print(\"Probability of picking the correct next token: \")\n",
    "print(\"> Neural net (exp(-CE)):\", \n",
    "    float(f\"{torch.exp(-eval_['test']):.4f}\"))\n",
    "print(\"> Uniform random guess (1/V):\", round(1/V,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2cea687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " “Exactly,” For said Alice; “but what to know.”   And what\n",
      "people half is Canto much happening and from like, but it\n",
      "was it back  out out like to fall on, “I’ll at they but her\n",
      "way I do is to my hear now.”   “See is their right the right\n",
      "to she left!” exclas she King the Queen, turning at  here\n",
      "way good outt.   “The exest of a shome gimmpent?” said\n",
      "Alice, aloud: “what o the Queen into of  reaces.”   “Yes, as\n",
      "you might to livery good round for to that do,” the Lory its\n",
      "make  out of she had not sai\n"
     ]
    }
   ],
   "source": [
    "net.eval()  # turns off dropout\n",
    "s = generate_text(net, n_new_tokens=500)\n",
    "print(textwrap.fill(s, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "473241e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only saving weights / parameter values\n",
    "PATH = \"./llm_weights.pth\"\n",
    "torch.save(net.state_dict(), PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6258d708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embeddings): Embedding(78, 64)\n",
       "  (position_embeddings): Embedding(256, 64)\n",
       "  (blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (lnorm1): LayerNorm()\n",
       "      (lnorm2): LayerNorm()\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (lnorm1): LayerNorm()\n",
       "      (lnorm2): LayerNorm()\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (lnorm1): LayerNorm()\n",
       "      (lnorm2): LayerNorm()\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (lnorm1): LayerNorm()\n",
       "      (lnorm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (output_lnorm): LayerNorm()\n",
       "  (output_layer): Linear(in_features=64, out_features=78, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: false\n",
    "\n",
    "net = Transformer(V, E)\n",
    "net.load_state_dict(torch.load(PATH, \n",
    "    map_location=device, weights_only=True))\n",
    "net = net.to(device)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2020d952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 232])\n",
      "torch.Size([1, 232, 78])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(999)\n",
    "\n",
    "X_ind, Y_ind = get_batch('train') # both are B x S \n",
    "\n",
    "# as an example,\n",
    "# we form initial token index sequence x \n",
    "# and y_w and y_l sequences of token inds that follow x\n",
    "# y_w > y_l\n",
    "\n",
    "x = X_ind[0,:24].unsqueeze(0) # 1 x 24\n",
    "y_w = X_ind[0,24:].unsqueeze(0) # 1 x (S-24)\n",
    "y_l = X_ind[1,24:].unsqueeze(0) # 1 x (S-24)\n",
    "\n",
    "# y_w follows x in the text, so should be more likely \n",
    "# y_l does not follow x, so should be less likely\n",
    "net.train()\n",
    "y_w_logits = net(torch.cat((x,y_w),1))[:, 24:]\n",
    "y_l_logits = net(torch.cat((x,y_l),1))[:, 24:]\n",
    "\n",
    "print(y_w.shape) # 1 x (S-24) \n",
    "print(y_w_logits.shape) # 1 x (S-24) x V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "38346718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_w = -8.336804389953613\n",
      "r_l = -8.146387100219727\n"
     ]
    }
   ],
   "source": [
    "# negative cross entropy\n",
    "r_w = -net.loss(y_w_logits, y_w)\n",
    "r_l = -net.loss(y_l_logits, y_l)\n",
    "\n",
    "print(\"r_w =\", r_w.item())\n",
    "print(\"r_l =\", r_l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f11998c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimPO loss: 0.7928813695907593\n"
     ]
    }
   ],
   "source": [
    "simpo_loss = -F.logsigmoid(r_w - r_l)\n",
    "\n",
    "optimizer_simpo = torch.optim.AdamW(net.parameters(), \n",
    "    lr=1e-6, weight_decay=0.1) # small learning rate\n",
    "\n",
    "optimizer_simpo.zero_grad()\n",
    "simpo_loss.backward() # accumulates back-propagated gradient\n",
    "optimizer_simpo.step() # parameter update (fine-tuning)\n",
    "\n",
    "print(\"SimPO loss:\", simpo_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86de1fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO loss: 0.6938948035240173\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# creating reference model\n",
    "net_ref = copy.deepcopy(net).to(device)\n",
    "\n",
    "# freezing the reference model\n",
    "net_ref.eval()\n",
    "for p in net_ref.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# forward pass under theta and reference models\n",
    "net.train()\n",
    "y_w_logits = net(torch.cat((x, y_w), 1))[:, 24:]\n",
    "y_l_logits = net(torch.cat((x, y_l), 1))[:, 24:]\n",
    "y_w_logits_ref = net_ref(torch.cat((x, y_w), 1))[:, 24:]\n",
    "y_l_logits_ref = net_ref(torch.cat((x, y_l), 1))[:, 24:]\n",
    "\n",
    "# compute per-token average log-probs = negative CE\n",
    "r_w_theta = -net.loss(y_w_logits, y_w) # theta reward for y_w\n",
    "r_l_theta = -net.loss(y_l_logits, y_l) # theta reward for y_l\n",
    "r_w_ref   = -net_ref.loss(y_w_logits_ref, y_w)  # reference reward for y_w\n",
    "r_l_ref   = -net_ref.loss(y_l_logits_ref, y_l)  # reference reward for y_l\n",
    "\n",
    "# DPO hyperparameter\n",
    "beta = 0.1\n",
    "\n",
    "# DPO y_w and y_l rewards\n",
    "r_w = beta*(r_w_theta - r_w_ref)\n",
    "r_l = beta*(r_l_theta - r_l_ref)\n",
    "\n",
    "# DPO loss\n",
    "dpo_loss = -F.logsigmoid(r_w - r_l)\n",
    "\n",
    "# optimizer\n",
    "optimizer_dpo = torch.optim.AdamW(net.parameters(), lr=1e-6, weight_decay=0.1)\n",
    "\n",
    "# backprop and update\n",
    "optimizer_dpo.zero_grad()\n",
    "dpo_loss.backward() # this step could be repeated multiple times to accumulate gradient across compared sequences\n",
    "optimizer_dpo.step() # only updates theta net parameters, not reference\n",
    "\n",
    "print(\"DPO loss:\", dpo_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2eeca1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prompt</th>\n",
       "      <td>Use the function to debug the given program an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chosen</th>\n",
       "      <td>One possible solution to prevent the segmentat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rejected</th>\n",
       "      <td>def debug_program(arr):\\n    n = len(arr)\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>8c94f83f-6a5a-5f8c-98a2-e242d7764938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          0\n",
       "prompt    Use the function to debug the given program an...\n",
       "chosen    One possible solution to prevent the segmentat...\n",
       "rejected  def debug_program(arr):\\n    n = len(arr)\\n   ...\n",
       "id                     8c94f83f-6a5a-5f8c-98a2-e242d7764938"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "data = load_dataset(\"jondurbin/py-dpo-v0.1\")\n",
    "\n",
    "# alternatively, load in the file directly\n",
    "# url = (\"https://huggingface.co/datasets/jondurbin/\"\n",
    "#          \"py-dpo-v0.1/resolve/main/py-dpo.parquet\"\n",
    "#          )\n",
    "# d = pd.read_parquet(url)\n",
    "# d = Dataset.from_pandas(d, split=\"train\")\n",
    "# data = DatasetDict()\n",
    "# data['train'] = d\n",
    "\n",
    "# single row as an example\n",
    "data['train'].to_pandas().head(1).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "049d0562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers==4.52.3\n",
    "# specific version set for reproducibility\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "set_seed(99)\n",
    "\n",
    "generator = pipeline('text-generation', \n",
    "        model=model_name, device=device)\n",
    "\n",
    "prompt = \"Give me a Python function to compute sample standard deviation based on a list of numbers, without numpy. Use [1, 2, 3, 4, 5] as an example.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "output = generator(\n",
    "    messages,\n",
    "    max_new_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68de5490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': [{'role': 'user', 'content': 'Give me a\n",
      "Python function to compute sample standard deviation based\n",
      "on a list of numbers, without numpy. Use [1, 2, 3, 4, 5] as\n",
      "an example.'}, {'role': 'assistant', 'content': \"To compute\n",
      "the sample standard deviation of a list of numbers in Python\n",
      "without using `numpy`, you can follow these steps:\\n\\n1.\n",
      "Sort the list.\\n2. Calculate the mean of the list.\\n3.\n",
      "Subtract the mean from each element and square the\n",
      "result.\\n4. Sum all the squared differences.\\n5. Divide the\n",
      "sum by the number of elements minus one (n-1) to get the\n",
      "variance.\\n6. Take the square root of the variance to get\n",
      "the standard deviation.\\n\\nHere's how you can implement this\n",
      "in Python:\\n\\n```python\\ndef\n",
      "calculate_sample_std_dev(numbers):\\n    # Step 1: Sort the\n",
      "list\\n    sorted_numbers = sorted(numbers)\\n    \\n    # Step\n",
      "2: Calculate the mean of the list\\n    n =\n",
      "len(sorted_numbers)\\n    mean = sum(sorted_numbers[:n]) /\n",
      "n\\n    \\n    # Step 3: Calculate the sum of squared\n",
      "differences from the mean\\n    total_sum_of_squares = sum((x\n",
      "- mean) ** 2 for x in sorted_numbers)\\n    \\n    # Step 4:\n",
      "Calculate the standard deviation\\n    std_deviation =\n",
      "total_sum_of_squares / (n - 1)\\n    \\n    return\n",
      "std_deviation\\n\\n# Example usage:\\nnumbers_list = [1, 2,\n",
      "3\"}]}]\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(str(output), 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc15ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute the sample standard deviation of a list of\n",
      "numbers in Python without using `numpy`, you can follow\n",
      "these steps:\n",
      "\n",
      "1. Sort the list.\n",
      "2. Calculate the mean of the list.\n",
      "3. Subtract the mean from each element and square the\n",
      "result.\n",
      "4. Sum all the squared differences.\n",
      "5. Divide the sum by the number of elements minus one (n-1)\n",
      "to get the variance.\n",
      "6. Take the square root of the variance to get the standard\n",
      "deviation.\n",
      "\n",
      "Here's how you can implement this in Python:\n",
      "\n",
      "```python\n",
      "def calculate_sample_std_dev(numbers):\n",
      "    # Step 1: Sort the list\n",
      "    sorted_numbers = sorted(numbers)\n",
      "\n",
      "    # Step 2: Calculate the mean of the list\n",
      "    n = len(sorted_numbers)\n",
      "    mean = sum(sorted_numbers[:n]) / n\n",
      "\n",
      "    # Step 3: Calculate the sum of squared differences from\n",
      "the mean\n",
      "    total_sum_of_squares = sum((x - mean) ** 2 for x in\n",
      "sorted_numbers)\n",
      "\n",
      "    # Step 4: Calculate the standard deviation\n",
      "    std_deviation = total_sum_of_squares / (n - 1)\n",
      "\n",
      "    return std_deviation\n",
      "\n",
      "# Example usage:\n",
      "numbers_list = [1, 2, 3\n"
     ]
    }
   ],
   "source": [
    "s = output[0]['generated_text'][1]['content']\n",
    "for si in s.split(\"\\n\"):\n",
    "    print(textwrap.fill(si, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9ca0141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 16:16, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.604600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.611200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.557300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.601800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.548700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install trl\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "set_seed(99)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\", \n",
    "            torch_dtype=torch.float32).to('cpu') \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "train_dataset = load_dataset(\"jondurbin/py-dpo-v0.1\", split=\"train\")\n",
    "\n",
    "training_args = DPOConfig(output_dir=\"Qwen2-0.5B-DPO\", \n",
    "                    fp16=False, bf16=False, use_cpu=True, \n",
    "                    num_train_epochs=1, max_steps=10, logging_steps=1,\n",
    "                    per_device_train_batch_size=4, \n",
    "                    gradient_accumulation_steps=4)\n",
    "trainer = DPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\n",
    "\n",
    "start_time = time.time()\n",
    "train_output = trainer.train()\n",
    "elapsed = time.time() - start_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2b02fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time = 18.29 mins\n",
      "Last update training loss: 0.5487\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total elapsed time = {elapsed/60:.2f} mins\")\n",
    "print(f\"Last update training loss: {trainer.state.log_history[-2].get('loss')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eefacc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python2llmsenv/lib/python3.13/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "set_seed(99)\n",
    "\n",
    "generator = pipeline('text-generation', \n",
    "        model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "output = generator(\n",
    "    messages,\n",
    "    max_new_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51c5897a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a Python function that computes the sample\n",
      "standard deviation using only basic arithmetic operations:\n",
      "```python\n",
      "import math\n",
      "\n",
      "def sample_std_dev(numbers):\n",
      "    mean = sum(numbers) / len(numbers)\n",
      "    variance = sum((x - mean) ** 2 for x in numbers) /\n",
      "(len(numbers) - 1)\n",
      "    return math.sqrt(variance)\n",
      "\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "print(sample_std_dev(numbers))\n",
      "```\n",
      "\n",
      "This function first calculates the mean of the input list by\n",
      "adding up all the numbers and dividing by the number of\n",
      "elements in the list. It then calculates the variance by\n",
      "subtracting each number from its mean and squaring the\n",
      "result. The variance is then divided by the square root of\n",
      "the number of elements in the list minus one, which gives us\n",
      "the sample standard deviation.\n",
      "Note that this implementation assumes that the input list\n",
      "contains only integers. If you want to handle floating-point\n",
      "numbers as well, you'll need to modify the code accordingly.\n"
     ]
    }
   ],
   "source": [
    "s = output[0]['generated_text'][1]['content']\n",
    "for si in s.split(\"\\n\"):\n",
    "    print(textwrap.fill(si, 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
